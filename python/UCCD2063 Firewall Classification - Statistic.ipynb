{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCCD2063 Machine Learning Classification Assignment\n",
    "The following is the Machine Learning algorithm designed to conduct classification tasks for multivariant attributes.\n",
    "\n",
    "**Contributors:**\n",
    "- Yee Zi Yang 18ACB02834\n",
    "- Lee Wai Hin 18ACB02736\n",
    "- Tan Jie Nan 18ACB02121\n",
    "\n",
    "**Classification Dataset:**\n",
    "- Firewall rules action on malicious data transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "### Contributor's Section:\n",
    "1. [Wai Hin's Code](#waihin_section)\n",
    "2. [Jie Nan's Code](#jienan_section)\n",
    "3. [Sammy's Code](#sammy_section)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The following is still under development and currently in alpha testing phase.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "print ('==> Loading specified dataset...\\n')\n",
    "firewall_data =  pd.read_csv(\"Firewall_data_sets.csv\")\n",
    "print ('==> Dataset loading completed.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "firewall_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_data.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_data.Action.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_data.corr() #correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"waihin_section\"></a>\n",
    "# WaiHin's Code Here\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb_encoder = LabelBinarizer()\n",
    "firewall_data.Action = lb_encoder.fit_transform(firewall_data.Action)\n",
    "#firewall_data\n",
    "#print(firewall_data.Action.value_counts().count)\n",
    "#print(firewall_data.describe())\n",
    "#print(type(firewall_data))\n",
    "firewall_data.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(firewall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = firewall_data.corr()\n",
    "corr_matrix['Action'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print('lala')\n",
    "#from pandas.plotting import scatter_matrix\n",
    "#print('lala')\n",
    "#selected_cols = ['Source Port', 'Destination Port', 'NAT Source Port']\n",
    "#print('lala')\n",
    "#scatter_matrix(firewall_data[selected_cols], figsize=(12,8)) \n",
    "#print('lala')\n",
    "#plt.show()\n",
    "#print('lala')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_data.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to separate the output (y) from input (X)\n",
    "X = firewall_data.drop('Action', axis = 1)\n",
    "y = firewall_data['Action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries first\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#Feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=8)\n",
    "fit = test.fit(X, y)\n",
    "\n",
    "#Summarize scores\n",
    "np.set_printoptions(precision = 5)\n",
    "print(fit.scores_)\n",
    "\n",
    "features = fit.transform(X)\n",
    "\n",
    "#Summarize selected features\n",
    "print(features[0:8, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = firewall_data.drop(['Elapsed Time (sec)', 'pkts_sent', 'pkts_received', 'Action'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Shape of original dataset, train', firewall_data.shape)\n",
    "print('X: shape=', X.shape, 'type=', type(X))\n",
    "print('y: shape=', y.shape, 'type=', type(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=30)\n",
    "\n",
    "print('full set shape =', X.shape)\n",
    "print('train shape=', X_train.shape)\n",
    "print('test shape=', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "print(type(X_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train.mean (axis = 0))\n",
    "print(X_train.std (axis = 0))\n",
    "\n",
    "print('>>> Mean of all columns:\\n', X_train.mean())\n",
    "print('\\n>>> Std of all columns:\\n', X_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Possible values of Action:\\n', lb_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('X_train shape: ', X_train.shape)\n",
    "print(type(y_train))\n",
    "print('y_train shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "#create a instance of DecisionTreeRegressor\n",
    "#tree_reg = ?\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "#Train the regressor model\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "#Predict on training set using trained regressor\n",
    "y_pred = tree_reg.predict(X_train)\n",
    "\n",
    "#Show the result of 10 random samples\n",
    "def show10results(y_train, y_pred):\n",
    "    print('Result for the 10 random samples: ')\n",
    "    selected = np.random.randint(0, len(y_train), 10)\n",
    "    for i in selected:\n",
    "        print('actual = {:7.0f} pred ={:7.0f}'.format(y_train[i], y_pred[i]))\n",
    "\n",
    "\n",
    "show10results(y_train, y_pred)\n",
    "\n",
    "#Compute and show the RMSE on training set\n",
    "tree_mse = mean_squared_error(y_pred, y_train)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print('RMSE = ', tree_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create a instance of RandomForestRegressor\n",
    "# forest_reg = ?\n",
    "forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the regressor model\n",
    "forest_reg.fit (X_train, y_train)\n",
    "\n",
    "# Predict on training set using trained regressor\n",
    "y_pred = forest_reg.predict(X_train)\n",
    "# Show the result of 10 random samples\n",
    "def show10results(y_train, y_pred):\n",
    "    print('Result for the 10 random samples:')\n",
    "    selected = np.random.randint(0, len(y_train), 10)\n",
    "    for i in selected:\n",
    "        print('actual = {:7.0f} pred ={:7.0f}'.format(y_train[i], y_pred[i]))\n",
    "\n",
    "show10results(y_train, y_pred)\n",
    "\n",
    "\n",
    "# Compute and show the RMSE on training set\n",
    "forest_mse = mean_squared_error(y_pred, y_train)\n",
    "forest_rmse= np.sqrt(forest_mse)\n",
    "print('RMSE =', forest_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_data.Action.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print('Scores: ', scores, '\\n')\n",
    "    print('Mean: ', scores.mean())\n",
    "    print('Standard deviation: ', scores.std())\n",
    "# perform k-fold cross-out validation\n",
    "# use cross_val_score on tree_reg\n",
    "k_fold_scores = cross_val_score(tree_reg, X_train, y_train, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "\n",
    "# convert NMSE to RMSE\n",
    "tree_rmse_scores = np.sqrt(-k_fold_scores)\n",
    "\n",
    "# use display_scores to show the results\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use cross_val_score on forest_reg\n",
    "\n",
    "k_fold_scores = cross_val_score(forest_reg, X_train, y_train, scoring ='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# covert NMSE to RMSE\n",
    "forest_rmse_scores = np.sqrt(-k_fold_scores)\n",
    "\n",
    "# use display_scores to show the results\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an action model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the SGD from sklearn\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Train using the training set\n",
    "sgd_clf = SGDClassifier (random_state = 42, max_iter = 5, tol = None)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd = sgd_clf.predict(X_train)\n",
    "y_pred_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection result for some randomly generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def peek_results(actual, predicted, num = 20):\n",
    "    print('actual | Predicted')\n",
    "    print('------------------')\n",
    "    for i in range(num):\n",
    "        sel = np.random.randint(0, len(y_train))\n",
    "        print(actual[sel], ' | ', predicted[sel])\n",
    "\n",
    "peek_results(y_train, y_pred_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measure: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# compute accuracy using accuracy_score and show the result\n",
    "train_acc = accuracy_score(y_train, y_pred_sgd)\n",
    "print('Training accuracy: {:.4f}'.format(train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring training accuracy using Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 5-fold cross validation accuracy and show the result\n",
    "# k_scores = ....\n",
    "\n",
    "k_scores = cross_val_score(sgd_clf, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "k_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_pred_sgd = sgd_clf.predict(X_train) \n",
    "print('Accuracy using prediction values: ', accuracy_score(y_train, y_train_pred_sgd))\n",
    "\n",
    "y_train_allzero = np.zeros(len(y_train), dtype=bool)\n",
    "print('Accuracy using prediction values: ', accuracy_score(y_train, y_train_allzero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measure: Confusion Matrix\n",
    "\n",
    "#### Perform cross-validated prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# perform cross-validation prediction using cross_val_predict\n",
    "y_pred_cv = cross_val_predict(sgd_clf, X_train, y_train, cv = 5)\n",
    "y_pred_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peek_results(y_train, y_pred_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix (y_train, y_pred_cv)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measure: Precision, Recall and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# compute and print the precision, recall and f1 score\n",
    "print('Precision = ', precision_score(y_train, y_pred_cv))\n",
    "print('Recall = ', recall_score(y_train, y_pred_cv))\n",
    "print('Precision = ', f1_score(y_train, y_pred_cv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measure: Precision-Recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_scores(actual, scores, num = 20):\n",
    "    print('actual | score')\n",
    "    print('---------------')\n",
    "    for i in range(num):\n",
    "        sel = np.random.randint(0, len(y_train))\n",
    "        print(actual[sel], ' | ', scores[sel])\n",
    "\n",
    "y_scores = sgd_clf.decision_function(X_train) # retrieve classification scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peek_scores(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the average cross-validated prediction score values for all samples\n",
    "# y_scores_cv = ....\n",
    "y_scores_cv = cross_val_predict(sgd_clf, X_train, y_train, cv = 5, method = 'decision_function')\n",
    "y_scores_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot precision and recall graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "# compute precision-recall pairs for different threshold values\n",
    "# precisions, recalls, thresholds = .....\n",
    "precisions, recalls, threholds = precision_recall_curve(y_train, y_scores_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, 'b-', linewidth=3)\n",
    "    plt.plot(np.linspace(0, 1, 20), np.linspace(1, 0, 20), 'k--')\n",
    "    plt.xlabel('Recall', fontsize = 16)\n",
    "    plt.ylabel('Precision', fontsize = 16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    \n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.title('Precision-Recall Graph (Training Set', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"waihin_section\"></a>\n",
    "# Jason's Code Here\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sammy_section\"></a>\n",
    "# Sammy's Code Here\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Initialization\n",
    "\n",
    "# Library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random seeding to ensure we have consistent results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "print ('==> Loading specified dataset...\\n')\n",
    "firewall_data =  pd.read_csv(\"Firewall_data_sets.csv\")\n",
    "print ('==> Dataset loading completed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Header\n",
    "firewall_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preview\n",
    "firewall_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Data Preview\n",
    "firewall_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output ( y ) attributes\n",
    "firewall_data.Action.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for this datasets consists of 4 unique values: Allow, Deny, Drop and Reset-both.<br>\n",
    "Thus we must consider a **multioutput regression** solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nearest neighbors for multioutput regression\n",
    "from sklearn.datasets import make_regression as MReg\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNReg\n",
    "\n",
    "# Testing multioutput regression with KN \n",
    "X, y = MReg(n_samples=1000, n_features=10, n_informative=5, n_targets=4, random_state=42, noise=0.5)\n",
    "\n",
    "y1 = y[:,0]\n",
    "y2 = y[:,1]\n",
    "y3 = y[:,2]\n",
    "y4 = y[:,3]\n",
    "\n",
    "print(\"Original\")\n",
    "plt.plot(X,y1,'bo',X,y2,'go',X,y3,'ro',X,y4,'yo')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "X_train,X_test,y_train,y_test = TTS(X,y,test_size=0.2)\n",
    "\n",
    "model = KNReg()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "y1 = y_pred[:,0]\n",
    "y2 = y_pred[:,1]\n",
    "y3 = y_pred[:,2]\n",
    "y4 = y_pred[:,3]\n",
    "\n",
    "print(\"Trained Data\")\n",
    "plt.plot(X_train,y1,'bo',X_train,y2,'go',X_train,y3,'ro',X_train,y4,'yo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the actual dataset output, y is in categorical attribute, we need to covnert it to a numerical attribute first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "encoder = OHE(handle_unknown='ignore')\n",
    "encoder.fit(firewall_data)\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_cat = pd.DataFrame(encoder.fit_transform(firewall_data[['Action']]).toarray())\n",
    "action_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = firewall_data.join(action_cat)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After joining the numerical value of 'Action', we can choose to drop the catergorical value of 'Action'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Action',axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}